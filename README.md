# Ejercicio de las 4 Vs del Big Data

Este ejercicio demuestra las cuatro caracterÃ­sticas fundamentales del Big Data mediante scripts de Python que simulan un producer y consumer.

## ğŸ“ Estructura del Proyecto

```
bigdataproblems/
â”œâ”€â”€ bigdata.py              # Script principal que coordina el ejercicio
â”œâ”€â”€ producer.py             # Genera datos segÃºn las 4 Vs
â”œâ”€â”€ consumer.py             # Procesa datos, muestra tiempos y genera grÃ¡ficas
â”œâ”€â”€ test_4vs.py             # Script para pruebas automÃ¡ticas
â”œâ”€â”€ stop_exercise.sh        # Script para detener el ejercicio fÃ¡cilmente
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â”œâ”€â”€ data/                   # Carpeta donde se almacenan los datos (se limpia automÃ¡ticamente)
â”œâ”€â”€ demo.png               # GrÃ¡fica de rendimiento generada automÃ¡ticamente
â””â”€â”€ README.md              # Este archivo
```

## ğŸš€ Uso del Ejercicio

### Comando BÃ¡sico
```bash
python3 bigdata.py --velocity true --volume true --variety true --veracity true
```

### Detener el Ejercicio
Si necesitas detener el ejercicio antes de tiempo:

**MÃ©todo 1 - Ctrl+C (recomendado):**
```bash
# Presiona Ctrl+C en la terminal donde se ejecuta
```

**MÃ©todo 2 - Script automÃ¡tico:**
```bash
./stop_exercise.sh
```

**MÃ©todo 3 - Manual:**
```bash
# Buscar procesos
ps aux | grep -E "(bigdata|producer|consumer)" | grep -v grep

# Terminar por PID
kill [PID_NUMBERS]
```

### Opciones Disponibles
- `--velocity true/false`: Activa la prueba de velocidad
- `--volume true/false`: Activa la prueba de volumen  
- `--variety true/false`: Activa la prueba de variedad
- `--veracity true/false`: Activa la prueba de veracidad

### Ejemplos de Uso

```bash
# Solo velocidad
python3 bigdata.py --velocity true

# Velocidad y volumen
python3 bigdata.py --velocity true --volume true

# Todas las Vs
python3 bigdata.py --velocity true --volume true --variety true --veracity true

# Solo variedad y veracidad
python3 bigdata.py --variety true --veracity true
```

## ğŸ“Š Las 4 Vs del Big Data

### 1. ğŸš€ Velocity (Velocidad)
- **Producer**: Escribe nÃºmeros cada X segundos, reduciendo el tiempo de espera cada 2 iteraciones
- **Consumer**: Procesa **UN SOLO nÃºmero por vez** para simular capacidad limitada de procesamiento
- **Efecto**: Se genera un **retraso creciente** - el consumer no puede mantener el ritmo del producer
- **Objetivo**: Demostrar el problema real del Big Data cuando los datos llegan mÃ¡s rÃ¡pido de lo que pueden procesarse

#### ğŸ“Š AnÃ¡lisis de Patrones en Velocity

El ejercicio de Velocity demuestra claramente el problema del **desbordamiento de capacidad** tÃ­pico del Big Data:

**PatrÃ³n Observable:**
| IteraciÃ³n | NÃºmeros en Archivo | NÃºmero Procesado | Retraso | Tendencia             |
| --------- | ------------------ | ---------------- | ------- | --------------------- |
| 15        | 15                 | 15               | 0       | Sincronizado          |
| 20        | 17                 | 16               | 1       | Inicio del retraso    |
| 22        | 30                 | 18               | 12      | Retraso exponencial   |
| 25        | 59                 | 21               | 38      | Crecimiento acelerado |
| 28        | 88                 | 24               | 64      | Sistema sobrecargado  |
| 30        | 107                | 26               | 81      | Backlog crÃ­tico       |

**Fases del Comportamiento:**
1. **Fase Inicial (0-15)**: Producer y Consumer sincronizados
2. **Punto de InflexiÃ³n (16-20)**: Producer comienza a acelerar
3. **Desbordamiento (20+)**: Retraso creciente exponencial
4. **SaturaciÃ³n (25+)**: Consumer no puede recuperar el ritmo

**MÃ©tricas Clave:**
- **Tiempo de Procesamiento**: Se mantiene constante (~0.2-1.5 ms)
- **Velocidad del Producer**: Aumenta progresivamente (sleep_time decrece)
- **Tasa de Retraso**: Crecimiento exponencial del backlog
- **Punto CrÃ­tico**: IteraciÃ³n ~20 donde el sistema se satura

**InterpretaciÃ³n Real:**
Este patrÃ³n simula escenarios tÃ­picos de Big Data como:
- ğŸ“± **Redes Sociales**: Posts llegando mÃ¡s rÃ¡pido que el anÃ¡lisis de sentimientos
- ğŸ“Š **IoT Sensors**: Datos de sensores superando capacidad de almacenamiento
- ğŸ›’ **E-commerce**: Transacciones en picos de trÃ¡fico sobrecargando el sistema
- ğŸ“ˆ **Trading**: Datos de mercado llegando mÃ¡s rÃ¡pido que el procesamiento de algoritmos

**FÃ³rmulas de AnÃ¡lisis:**
```
Retraso(t) = NÃºmeros_en_Archivo(t) - NÃºmeros_Procesados(t)
Tasa_ProducciÃ³n(t) = NÃºmeros_Generados / Tiempo_Transcurrido
Tasa_Consumo = 1 nÃºmero/segundo (constante)
SaturaciÃ³n = Cuando Tasa_ProducciÃ³n > Tasa_Consumo
```

**Lecciones Aprendidas:**
1. **Escalabilidad Horizontal**: Necesidad de mÃºltiples consumers paralelos
2. **Buffering Inteligente**: Sistemas de cola para gestionar picos
3. **PriorizaciÃ³n**: Procesar datos crÃ­ticos primero
4. **Monitoreo Proactivo**: Detectar saturaciÃ³n antes del colapso

### ğŸ“ˆ AnÃ¡lisis Visual AutomÃ¡tico

Al finalizar, se generan grÃ¡ficas que muestran:
- **Tendencia del Retraso**: Curva exponencial que demuestra la saturaciÃ³n
- **Punto de InflexiÃ³n**: Momento exacto donde el sistema se sobrecarga  
- **PatrÃ³n de Escalabilidad**: CÃ³mo afecta el volumen al rendimiento
- **MÃ©tricas de Throughput**: Capacidad de procesamiento en tiempo real

### 2. ğŸ“ˆ Volume (Volumen)  
- **Producer**: Incrementa la cantidad de nÃºmeros escritos cada 2 iteraciones (1, 2, 3, 4...)
- **Consumer**: Procesa volÃºmenes crecientes de datos completos cada vez
- **Objetivo**: Mostrar cÃ³mo el aumento del volumen impacta los tiempos de procesamiento

#### ğŸ“Š AnÃ¡lisis de Patrones en Volume

El ejercicio de Volume demuestra el **crecimiento controlado** y su impacto en el rendimiento:

**PatrÃ³n de Crecimiento Exponencial:**
| IteraciÃ³n | NÃºmeros Totales | Volumen AÃ±adido | Tiempo Proc. | Eficiencia      |
| --------- | --------------- | --------------- | ------------ | --------------- |
| 0         | 1               | 1               | 0.10 ms      | Ã“ptima          |
| 3         | 22              | +20             | 0.46 ms      | Buena           |
| 7         | 192             | +150            | 0.63 ms      | Aceptable       |
| 11        | 542             | +200            | 0.73 ms      | Moderada        |
| 13        | 742             | +200            | 1.02 ms      | **Impacto**     |
| 15        | 1,742           | +1,000          | 1.46 ms      | **DegradaciÃ³n** |
| 17        | 2,742           | +1,000          | 1.90 ms      | **CrÃ­tica**     |
| 21        | 5,142           | +1,200          | 2.67 ms      | **SaturaciÃ³n**  |

**Fases del Volumen:**
1. **Micro Datos (1-50)**: Procesamiento instantÃ¡neo, sin impacto
2. **Datos PequeÃ±os (50-500)**: Crecimiento visible, rendimiento estable  
3. **Datos Medianos (500-1K)**: Primer impacto en tiempo de procesamiento
4. **Big Data (1K-5K+)**: DegradaciÃ³n clara, necesidad de optimizaciÃ³n
5. **Volumen CrÃ­tico (5K+)**: SaturaciÃ³n del sistema, tiempos exponenciales

**MÃ©tricas de Rendimiento:**
- **Escalabilidad**: Limitada (degradaciÃ³n visible con volÃºmenes >1K)
- **Punto de InflexiÃ³n**: ~750 nÃºmeros (tiempo >1ms)
- **DegradaciÃ³n**: 26x mÃ¡s lento (0.10ms â†’ 2.67ms)
- **Throughput**: Disminuye exponencialmente con volumen alto
- **SaturaciÃ³n**: Evidente en volÃºmenes >5K nÃºmeros

**InterpretaciÃ³n Real:**
Este patrÃ³n simula escenarios como:
- ğŸ“Š **Data Warehouses**: Carga incremental diaria/semanal
- ğŸ“ˆ **Analytics**: Datasets crecientes en anÃ¡lisis histÃ³rico
- ğŸ—„ï¸ **Backups**: VolÃºmenes de respaldo incrementales
- ğŸ“‹ **Batch Processing**: Procesamiento por lotes de tamaÃ±o creciente

**Lecciones del Volume:**
1. **Escalabilidad Vertical**: Sistemas que manejan volumen sin degradaciÃ³n
2. **PlanificaciÃ³n de Capacidad**: Predecir recursos segÃºn crecimiento
3. **OptimizaciÃ³n de Memoria**: GestiÃ³n eficiente de datasets grandes
4. **Arquitectura Batch**: DiseÃ±o para procesar volÃºmenes variables

### ğŸ“ˆ AnÃ¡lisis Visual AutomÃ¡tico

Las grÃ¡ficas generadas revelan:
- **Curva de Escalabilidad**: RelaciÃ³n no-lineal entre volumen y tiempo
- **Punto de SaturaciÃ³n**: Umbral donde el rendimiento se degrada
- **Patrones de Eficiencia**: IdentificaciÃ³n de volÃºmenes Ã³ptimos
- **Throughput Variable**: CÃ³mo cambia la eficiencia con el tamaÃ±o### 3. ğŸ”„ Variety (Variedad)
- **Producer**: Escribe los mismos datos en mÃºltiples formatos (JSON, CSV, TXT)
- **Consumer**: Lee y procesa diferentes formatos, mostrando resultados por tipo
- **Objetivo**: Demostrar la complejidad de manejar mÃºltiples formatos de datos

### 4. âš ï¸ Veracity (Veracidad)
- **Producer**: Introduce errores aleatorios en algunos valores (10% de probabilidad)
- **Consumer**: Detecta discrepancias entre archivos del mismo lote
- **Objetivo**: Mostrar problemas de calidad y consistencia de datos

## ğŸ“‹ Funcionamiento Detallado

### InicializaciÃ³n
1. El script `bigdata.py` limpia la carpeta `data/` automÃ¡ticamente
2. Inicia el `producer.py` en segundo plano
3. Ejecuta el `consumer.py` en primer plano para mostrar resultados

### GestiÃ³n de Archivos
- **Producer**: Siempre escribe a los mismos archivos (`data.txt`, `data.csv`, `data.json`)
- **TXT/CSV**: AÃ±ade nuevos nÃºmeros al final del archivo (append)
- **JSON**: Actualiza el array completo de nÃºmeros preservando los anteriores
- **Consumer**: Solo procesa cuando detecta cambios en el tamaÃ±o de los archivos

### Producer (Generador de Datos)
- Genera nÃºmeros secuenciales (1, 2, 3, 4...)
- Aplica las transformaciones segÃºn las Vs activadas
- **Escribe siempre a los mismos archivos** (`data.txt`, `data.csv`, `data.json`) aÃ±adiendo datos

### Consumer (Procesador de Datos)
- Monitorea los archivos fijos en `data/` continuamente
- Procesa solo cuando detecta cambios en el tamaÃ±o de los archivos
- **Muestra Ãºnicamente los tiempos de procesamiento** como se solicitÃ³
- Detecta discrepancias cuando veracity estÃ¡ activo

## â±ï¸ Salida del Consumer

El consumer muestra:
- **Tiempo de procesamiento en milisegundos**
- NÃºmero de archivos procesados
- Suma total de nÃºmeros
- Para variety: resultados separados por formato
- Para veracity: alertas de discrepancias detectadas

## ğŸ“Š GrÃ¡ficas AutomÃ¡ticas

Al finalizar cada ejercicio, se genera automÃ¡ticamente una **grÃ¡fica de 4 paneles** que muestra:

1. **Tiempo de Procesamiento**: EvoluciÃ³n temporal del rendimiento
2. **Volumen de Datos**: ComparaciÃ³n entre nÃºmeros en archivo vs procesados
3. **AnÃ¡lisis EspecÃ­fico**:
   - **Velocity**: Retraso creciente entre producer y consumer
   - **Volume**: Escalabilidad (volumen vs tiempo de procesamiento)
4. **Throughput**: NÃºmeros procesados por milisegundo

### Archivos Generados
- `demo.png` - GrÃ¡fica de rendimiento con 4 paneles de anÃ¡lisis visual

**Nota importante**: Para asegurar que la grÃ¡fica se genere correctamente, usa **Ctrl+C** para terminar el ejercicio y espera a ver el mensaje "âœ… GrÃ¡fica guardada exitosamente como: demo.png". Las grÃ¡ficas se guardan automÃ¡ticamente al finalizar el anÃ¡lisis.

## ğŸ› ï¸ Requisitos

- Python 3.6+
- LibrerÃ­as listadas en `requirements.txt`

### InstalaciÃ³n de Dependencias

```bash
pip install -r requirements.txt
```

O instalar manualmente:
```bash
pip install matplotlib seaborn numpy pandas
```

## ğŸ§ª Script de Pruebas AutomÃ¡ticas

Para probar cada V individualmente de manera automÃ¡tica:

```bash
python3 test_4vs.py
```

Este script ejecuta 5 pruebas secuenciales:
1. **Velocity** - Solo velocidad variable (10s)
2. **Volume** - Solo volumen creciente (8s)  
3. **Variety** - Solo mÃºltiples formatos (6s)
4. **Veracity** - DetecciÃ³n de errores (10s)
5. **All** - Las 4 Vs combinadas (12s)

## ğŸ”§ PersonalizaciÃ³n

Puedes modificar los siguientes parÃ¡metros en el cÃ³digo:

### En `producer.py`:
- `sleep_time`: Tiempo inicial entre iteraciones (default: 2.0s)
- `error_probability`: Probabilidad de errores para veracity (default: 0.1)
- Tipos de errores introducidos

### En `consumer.py`:
- Intervalo de monitoreo (default: 1s)
- Formatos de archivo soportados

## ğŸ“ Ejemplos de Salida

### Ejemplo: Solo Velocity
```
ğŸ“‹ IteraciÃ³n 15:  [SINCRONIZADO]
â±ï¸  Tiempo de procesamiento: 0.94 ms
ğŸ“„ data.txt: 15 nÃºmeros en el archivo
ğŸ”¢ NÃºmero procesado: 15
ğŸ“Š Total procesados hasta ahora: 15

ğŸ“‹ IteraciÃ³n 20:  [INICIO DEL RETRASO]
â±ï¸  Tiempo de procesamiento: 1.44 ms
ğŸ“„ data.txt: 17 nÃºmeros en el archivo
ğŸ”¢ NÃºmero procesado: 16
ğŸ“Š Total procesados hasta ahora: 16
         âš ï¸ Retraso: 1 nÃºmero

ğŸ“‹ IteraciÃ³n 25:  [SATURACIÃ“N DEL SISTEMA]
â±ï¸  Tiempo de procesamiento: 1.20 ms
ğŸ“„ data.txt: 59 nÃºmeros en el archivo
ğŸ”¢ NÃºmero procesado: 21
ğŸ“Š Total procesados hasta ahora: 21
         ğŸš¨ Retraso: 38 nÃºmeros - SISTEMA SOBRECARGADO

ğŸ“‹ IteraciÃ³n 30:  [COLAPSO INMINENTE]
â±ï¸  Tiempo de procesamiento: 0.59 ms
ğŸ“„ data.txt: 107 nÃºmeros en el archivo
ğŸ”¢ NÃºmero procesado: 26
ğŸ“Š Total procesados hasta ahora: 26
         ğŸ’¥ Retraso: 81 nÃºmeros - BACKLOG CRÃTICO
```

### Ejemplo: Solo Volume
```
ğŸ“‹ IteraciÃ³n 0:  [MICRO DATOS]
â±ï¸  Tiempo de procesamiento: 0.10 ms
ğŸ“ Archivos procesados: 1
ğŸ”¢ Suma total: 1
ğŸ“Š NÃºmeros totales procesados: 1

ğŸ“‹ IteraciÃ³n 7:  [DATOS PEQUEÃ‘OS]
â±ï¸  Tiempo de procesamiento: 0.63 ms
ğŸ“ Archivos procesados: 1
ğŸ”¢ Suma total: 18,528
ğŸ“Š NÃºmeros totales procesados: 192
         ğŸ“ˆ Salto exponencial: +150 nÃºmeros

ğŸ“‹ IteraciÃ³n 13:  [PUNTO DE INFLEXIÃ“N]
â±ï¸  Tiempo de procesamiento: 1.02 ms
ğŸ“ Archivos procesados: 1
ğŸ”¢ Suma total: 275,653
ğŸ“Š NÃºmeros totales procesados: 742
         âš ï¸ DegradaciÃ³n: >1ms por primera vez

ï¿½ IteraciÃ³n 17:  [BIG DATA VISIBLE]
â±ï¸  Tiempo de procesamiento: 1.90 ms
ğŸ“ Archivos procesados: 1
ğŸ”¢ Suma total: 3,760,653
ğŸ“Š NÃºmeros totales procesados: 2,742
         ğŸš¨ Impacto: 19x mÃ¡s lento que el inicio

ğŸ“‹ IteraciÃ³n 21:  [SATURACIÃ“N DEL SISTEMA]
â±ï¸  Tiempo de procesamiento: 2.67 ms
ğŸ“ Archivos procesados: 1
ğŸ”¢ Suma total: 13,222,653
ğŸ“Š NÃºmeros totales procesados: 5,142
         ï¿½ CrÃ­tico: 26x mÃ¡s lento - OPTIMIZACIÃ“N NECESARIA
```

### Variety + Veracity
```
ğŸ“‹ IteraciÃ³n 2:

â±ï¸  Tiempo de procesamiento: 8.42 ms
ğŸ“ Archivos nuevos procesados: 3
   .txt: Suma=156, NÃºmeros=12, Archivos=1
   .csv: Suma=156, NÃºmeros=12, Archivos=1  
   .json: Suma=1156, NÃºmeros=12, Archivos=1
âš ï¸  DISCREPANCIA DETECTADA: Las sumas no coinciden entre formatos
```

## ğŸ¯ Objetivos de Aprendizaje

Este ejercicio ayuda a entender:
1. **Velocity**: CÃ³mo la velocidad variable afecta el procesamiento en tiempo real
2. **Volume**: El impacto del crecimiento de datos en el rendimiento
3. **Variety**: La complejidad de integrar mÃºltiples formatos
4. **Veracity**: La importancia de la calidad y consistencia de los datos

Â¡Experimenta con diferentes combinaciones de las 4 Vs para ver cÃ³mo interactÃºan entre sÃ­!
